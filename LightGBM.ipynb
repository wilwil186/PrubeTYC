{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# Predicción de Altas de Tarjetas de Crédito\n",
        "### Prueba Técnica - Selección de Variables Significativas y Optimización del Umbral\n",
        "\n",
        "En este notebook se desarrolla un modelo de Machine Learning para predecir si un cliente adquirirá una tarjeta de crédito (`alta_tdc`).\n",
        "El proceso se divide en dos etapas principales:\n",
        "\n",
        "1. **Selección de Variables Significativas:** Se entrena un modelo base para extraer la importancia de cada variable y, posteriormente, se optimiza el umbral de selección para elegir las variables más relevantes.\n",
        "2. **Construcción y Evaluación del Modelo Final:** Se construye un pipeline final que incluye preprocesamiento, balanceo de clases (SMOTE) y entrenamiento de un LightGBMClassifier con hiperparámetros predeterminados.\n",
        "\n",
        "El objetivo es obtener un modelo robusto y, al final, mostrar únicamente el average_precision_score y la precisión."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "objective",
      "metadata": {},
      "source": [
        "## Objetivo y Entregables\n",
        "\n",
        "**Objetivo:** Desarrollar un modelo predictivo utilizando únicamente las variables significativas, optimizando el umbral de selección y entrenando un modelo con parámetros predeterminados, de manera que se impriman únicamente el average_precision_score y la precisión.\n",
        "\n",
        "**Entregables:**\n",
        "- **Notebook (.ipynb):** Con el desarrollo del modelo, incluyendo la justificación de las decisiones tomadas y una reflexión sobre posibles mejoras o próximos pasos.\n",
        "- **Archivo de predicciones (.csv):** Con las columnas `ID_cliente` y `alta_tdc` (1 si el cliente adquiere la tarjeta, 0 en caso contrario)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score, average_precision_score, f1_score\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "import lightgbm as lgb  # Se utiliza LightGBM en lugar de RandomForestClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "load_data_preprocess",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset de entrenamiento cargado, dimensiones: (12012, 165)\n",
            "Columnas numéricas: ['cust_age_number', 'customer_seniority_number', 'main_city_type', 'marital_status_type', 'address_relationship_type', 'adult_dependants_number', 'target_type', 'customer_asset_amount', 'customer_liability_amount', 'customer_status_type', 'month_withdrawals_number', 'month_payment_number', 'month_purchase_number', 'm_exit_amount', 'm_entry_amount', 'mth_mbl_app_qry_number', 'mth_mbl_app_mnty_tr_number', 'mth_mbl_app_non_mnty_tr_number', 'month_atm_queries_number', 'atm_made_fin_trans_number', 'atm_made_nfin_tran_number', 'inquiries_digi_channel_number', 'dig_financial_trans_tot_number', 'dig_non_fin_trans_tot_number', 'mth_co_netcash_queries_number', 'mth_co_netc_non_mnty_tr_number', 'mth_co_netcash_mnty_tr_number', 'mth_ppl_netcash_queries_number', 'mth_ppl_netcash_mnty_tr_number', 'mth_pplnetc_non_mnty_tr_number', 'month_h2h_queries_number', 'month_h2h_mnty_tr_number', 'month_h2h_non_mnty_tr_number', 'month_webpage_queries_number', 'mth_webpage_mnty_tr_number', 'mth_webpage_non_mnty_tr_number', 'mth_ext_pos_queries_number', 'mth_ext_pos_mnty_tr_number', 'mth_ext_pos_non_mnty_tr_number', 'mth_ext_ntwrk_queries_number', 'mth_ext_ntwrk_mnty_tr_number', 'm_ext_ntwrk_non_mnty_tr_number', 'mth_banknet_queries_number', 'mth_banknet_mnty_tr_number', 'mth_banknet_non_mnty_tr_number', 'mth_third_party_queries_number', 'month_thpty_mnty_tr_number', 'month_thpty_non_mnty_tr_number', 'recv_payrl_transactions_number', 'current_payroll_incm_amount', 'payroll_type', 'instlmnt_1_bill_average_amount', 'cr_card_bill_average_amount', 'instlmnt_1_tr_average_per', 'instlmnt_1_transactions_type', 'saving_prdt_avg_bal_amount', 'fix_saving_prdt_avg_bal_amount', 'cdt_prdt_avg_balance_amount', 'consumer_cash_balance_amount', 'current_asset_amount', 'avg_balance_fund_amount', 'mtge_prdt_avg_balance_amount', 'bbva_payrl_oblg_tot_bal_amount', 'avg_cr_card_balance_amount', 'vehicle_credit_average_amount', 'lt_saving_prdt_avg_bal_amount', 'save_account_type', 'fixed_savings_tenure_type', 'term_cdt_tenure_type', 'tenure_consm_product_type', 'ownership_ca_type', 'funds_ownership_ind_type', 'ownership_mloan_type', 'libranza_tenure_mark_type', 'credit_card_ownshp_ind_type', 'vehicle_cr_tenure_mark_type', 'long_term_savings_tenure_type', 'new_segment_id', 'customer_products_number', 'cr_dbt_card_agric_expns_amount', 'cr_dbt_card_agric_op_number', 'cr_dbt_card_adv_expns_amount', 'cr_dbt_card_adv_op_number', 'cr_dbt_card_veh_expns_amount', 'cr_dbt_card_veh_op_number', 'cr_dbt_cd_beauty_expns_amount', 'cr_dbt_card_beauty_op_number', 'cr_dbt_cd_casino_expns_amount', 'cr_dbt_card_casino_op_number', 'cr_dbt_cd_cns_expns_amount', 'cr_dbt_card_cns_op_number', 'cr_dbt_cd_sport_expns_amount', 'cr_dbt_card_sport_op_number', 'cr_dbt_cd_digital_expns_amount', 'cr_dbt_cd_digital_op_number', 'cr_dbt_card_educ_expns_amount', 'cr_dbt_card_educ_op_number', 'cr_dbt_cd_co_stmp_expns_amount', 'cr_dbt_cd_co_stamp_op_number', 'cr_dbt_card_fcg_expns_amount', 'cr_dbt_card_fcg_op_number', 'cr_dbt_card_fndat_expns_amount', 'cr_dbt_card_fndat_op_number', 'cr_dbt_cd_funeral_expns_amount', 'cr_dbt_card_funeral_op_number', 'cr_dbt_card_govt_expns_amount', 'cr_dbt_card_govt_op_number', 'cr_dbt_card_home_expns_amount', 'cr_dbt_card_home_op_number', 'cr_dbt_card_inds_expns_amount', 'cr_dbt_card_inds_op_number', 'cr_dbt_card_re_expns_amount', 'cr_dbt_card_re_op_number', 'cr_dbt_card_jwlry_expns_amount', 'cr_dbt_card_jwlry_op_number', 'cr_dbt_cd_lbrtry_expns_amount', 'cr_dbt_cd_laboratory_op_number', 'cr_dbt_cd_lqr_str_expns_amount', 'cr_dbt_cd_lqr_str_op_number', 'cr_dbt_card_clng_expns_amount', 'cr_dbt_card_clng_op_number', 'cr_dbt_card_maint_expns_amount', 'cr_dbt_card_maint_op_number', 'cr_dbt_card_pets_expns_amount', 'cr_dbt_card_pets_op_number', 'cr_dbt_card_med_expns_amount', 'cr_dbt_card_med_op_number', 'cr_dbt_cd_courier_expns_amount', 'cr_dbt_card_courier_op_number', 'cr_dbt_card_misc_expns_amount', 'cr_debit_card_misc_op_number', 'cr_dbt_card_motel_expns_amount', 'cr_debit_card_motel_op_number', 'cr_dbt_cd_others_expns_amount', 'cr_debit_card_others_op_number', 'cr_dbt_cd_sttnry_expns_amount', 'cr_debit_card_sttnry_op_number', 'cr_dbt_cd_parkg_expns_amount', 'cr_debit_card_parkg_op_number', 'cr_dbt_cd_bakery_expns_amount', 'cr_debit_card_bakery_op_number', 'cr_dbt_cd_recyc_expns_amount', 'cr_debit_card_recyc_op_number', 'cr_dbt_cd_rstrnt_expns_amount', 'cr_debit_card_rstrnt_op_number', 'cr_dbt_cd_ins_expns_amount', 'cr_debit_card_ins_op_number', 'cr_dbt_cd_pub_srv_expns_amount', 'cr_debit_cd_pub_srv_op_number', 'cr_dbt_card_spmk_expns_amount', 'cr_debit_card_spmk_op_number', 'cr_dbt_card_tech_expns_amount', 'cr_debit_card_tech_op_number', 'cr_dbt_cd_tourism_expns_amount', 'cr_debit_cd_tourism_op_number', 'cr_dbt_cd_clothes_expns_amount', 'cr_debit_cd_clothes_op_number', 'cr_dbt_card_prvs_expns_amount', 'cr_debit_card_prvs_op_number']\n",
            "Columnas categóricas: ['town_id', 'personal_type', 'collective_name', 'audit_date']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\wilson.hernandez\\AppData\\Local\\Temp\\ipykernel_1716\\3512137320.py:2: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_df = pd.read_csv('train_df.csv', delimiter=';', thousands='.', decimal=',')\n"
          ]
        }
      ],
      "source": [
        "# Cargar el dataset de entrenamiento\n",
        "train_df = pd.read_csv('train_df.csv', delimiter=';', thousands='.', decimal=',')\n",
        "print('Dataset de entrenamiento cargado, dimensiones:', train_df.shape)\n",
        "train_df.head()\n",
        "\n",
        "# Definir variables objetivo e identificación\n",
        "target = 'alta_tdc'\n",
        "id_col = 'identificador_cliente'\n",
        "\n",
        "# Separar características y target\n",
        "X = train_df.drop([target, id_col], axis=1)\n",
        "y = train_df[target]\n",
        "\n",
        "# Identificar columnas numéricas y categóricas\n",
        "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
        "cat_cols = X.select_dtypes(exclude=['number']).columns.tolist()\n",
        "\n",
        "print('Columnas numéricas:', num_cols)\n",
        "print('Columnas categóricas:', cat_cols)\n",
        "\n",
        "# Convertir variables categóricas a string\n",
        "X[cat_cols] = X[cat_cols].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model_base_intro",
      "metadata": {},
      "source": [
        "## Modelo Base y Cálculo de Importancia de Variables con LightGBM\n",
        "\n",
        "Se construye el pipeline base utilizando LightGBM para entrenar el modelo y extraer la importancia de cada variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "model_base_code",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
            "[WinError 2] El sistema no puede encontrar el archivo especificado\n",
            "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
            "  warnings.warn(\n",
            "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
            "    cpu_info = subprocess.run(\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\ProgramData\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\ProgramData\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "  File \"c:\\ProgramData\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
            "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005134 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 10089\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 216\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "Importancia de las variables:\n",
            "customer_seniority_number      147\n",
            "cust_age_number                134\n",
            "saving_prdt_avg_bal_amount     132\n",
            "customer_liability_amount      127\n",
            "current_payroll_incm_amount    123\n",
            "customer_asset_amount          105\n",
            "mth_mbl_app_qry_number          89\n",
            "month_withdrawals_number        83\n",
            "m_entry_amount                  79\n",
            "month_purchase_number           72\n",
            "dtype: int32\n"
          ]
        }
      ],
      "source": [
        "# Pipeline para variables numéricas\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline para variables categóricas\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "# Combinar ambos pipelines\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, num_cols),\n",
        "    ('cat', categorical_transformer, cat_cols)\n",
        "])\n",
        "\n",
        "# Crear pipeline base: preprocesamiento + LightGBM\n",
        "pipeline_base = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', lgb.LGBMClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# División en entrenamiento y prueba\n",
        "X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "pipeline_base.fit(X_train_base, y_train_base)\n",
        "\n",
        "# Extraer el clasificador entrenado\n",
        "model_base = pipeline_base.named_steps['classifier']\n",
        "\n",
        "# Obtener nombres de columnas tras preprocesamiento\n",
        "onehot_feature_names = pipeline_base.named_steps['preprocessor']\\\n",
        "    .named_transformers_['cat']\\\n",
        "    .named_steps['onehot'].get_feature_names_out(cat_cols)\n",
        "feature_names = num_cols + list(onehot_feature_names)\n",
        "\n",
        "# Calcular importancias\n",
        "importances = model_base.feature_importances_\n",
        "feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "print('Importancia de las variables:')\n",
        "print(feat_imp.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "umbral_intro",
      "metadata": {},
      "source": [
        "## Optimización del Umbral de Importancia\n",
        "\n",
        "Se recorre un rango de umbrales para seleccionar las variables más relevantes utilizando LightGBM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "umbral_code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004825 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 9812\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003029 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8412\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 151\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003228 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 6343\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 122\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001204 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4840\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 108\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000823 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4282\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 103\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000669 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3347\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 97\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000508 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3317\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000520 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2645\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 13\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000384 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2363\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000381 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2108\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000330 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1853\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 9\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1343\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000225 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1167\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 6\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000546 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1167\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 6\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000194 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 912\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000196 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 912\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000176 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 657\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 4\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000133 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 402\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 3\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "[LightGBM] [Info] Number of positive: 982, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000076 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 47\n",
            "[LightGBM] [Info] Number of data points in the train set: 9609, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102196 -> initscore=-2.173061\n",
            "[LightGBM] [Info] Start training from score -2.173061\n",
            "Mejor umbral: 23.210526315789473\n",
            "Variables numéricas seleccionadas: ['cust_age_number', 'customer_seniority_number', 'main_city_type', 'customer_asset_amount', 'customer_liability_amount', 'month_withdrawals_number', 'month_purchase_number', 'm_exit_amount', 'm_entry_amount', 'mth_mbl_app_qry_number', 'mth_mbl_app_mnty_tr_number', 'month_atm_queries_number', 'atm_made_fin_trans_number', 'mth_ppl_netcash_queries_number', 'month_h2h_mnty_tr_number', 'mth_ext_pos_mnty_tr_number', 'current_payroll_incm_amount', 'saving_prdt_avg_bal_amount', 'consumer_cash_balance_amount', 'bbva_payrl_oblg_tot_bal_amount', 'avg_cr_card_balance_amount', 'new_segment_id', 'cr_dbt_cd_digital_expns_amount', 'cr_dbt_cd_courier_expns_amount', 'cr_dbt_cd_rstrnt_expns_amount', 'cr_dbt_card_spmk_expns_amount', 'cr_dbt_card_tech_expns_amount']\n",
            "Variables categóricas seleccionadas: ['town_id']\n"
          ]
        }
      ],
      "source": [
        "# Definir un rango de umbrales (de 0 al máximo de importancias)\n",
        "umbral_values = np.linspace(0, feat_imp.max(), num=20)\n",
        "\n",
        "best_umbral = None\n",
        "best_score = 0\n",
        "best_num_cols = None\n",
        "best_cat_cols = None\n",
        "\n",
        "for umbral in umbral_values:\n",
        "    features_temp = feat_imp[feat_imp > umbral].index.tolist()\n",
        "    \n",
        "    num_cols_temp = [col for col in num_cols if col in features_temp]\n",
        "    \n",
        "    onehot_all_names = pipeline_base.named_steps['preprocessor']\\\n",
        "        .named_transformers_['cat']\\\n",
        "        .named_steps['onehot'].get_feature_names_out(cat_cols)\n",
        "    cat_cols_temp = []\n",
        "    for col in cat_cols:\n",
        "        dummies = [dummy for dummy in onehot_all_names if dummy.startswith(col + '_')]\n",
        "        if any(dummy in features_temp for dummy in dummies):\n",
        "            cat_cols_temp.append(col)\n",
        "    \n",
        "    selected_cols_temp = num_cols_temp + cat_cols_temp\n",
        "    if len(selected_cols_temp) == 0:\n",
        "        continue\n",
        "    \n",
        "    X_selected_temp = X[selected_cols_temp]\n",
        "    X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(X_selected_temp, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    numeric_transformer_sel = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    categorical_transformer_sel = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "    preprocessor_sel = ColumnTransformer(transformers=[\n",
        "        ('num', numeric_transformer_sel, [col for col in selected_cols_temp if col in num_cols]),\n",
        "        ('cat', categorical_transformer_sel, [col for col in selected_cols_temp if col in cat_cols])\n",
        "    ])\n",
        "    clf_temp = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor_sel),\n",
        "        ('classifier', lgb.LGBMClassifier(n_estimators=100, random_state=42))\n",
        "    ])\n",
        "    \n",
        "    clf_temp.fit(X_train_temp, y_train_temp)\n",
        "    y_pred_temp = clf_temp.predict(X_test_temp)\n",
        "    \n",
        "    # Utilizamos el F1 Score internamente para la optimización (no se imprime)\n",
        "    score = 0\n",
        "    try:\n",
        "        score = f1_score(y_test_temp, y_pred_temp, pos_label=1)\n",
        "    except Exception as e:\n",
        "        pass\n",
        "    \n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_umbral = umbral\n",
        "        best_num_cols = num_cols_temp\n",
        "        best_cat_cols = cat_cols_temp\n",
        "\n",
        "print('Mejor umbral:', best_umbral)\n",
        "print('Variables numéricas seleccionadas:', best_num_cols)\n",
        "print('Variables categóricas seleccionadas:', best_cat_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "final_pipeline_intro",
      "metadata": {},
      "source": [
        "## Construcción del Pipeline Final con Variables Seleccionadas y LightGBM\n",
        "\n",
        "Utilizando el umbral óptimo se define el pipeline final que incluye SMOTE para balancear las clases y LightGBM como clasificador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "final_pipeline_code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columnas seleccionadas para el modelo final: ['cust_age_number', 'customer_seniority_number', 'main_city_type', 'customer_asset_amount', 'customer_liability_amount', 'month_withdrawals_number', 'month_purchase_number', 'm_exit_amount', 'm_entry_amount', 'mth_mbl_app_qry_number', 'mth_mbl_app_mnty_tr_number', 'month_atm_queries_number', 'atm_made_fin_trans_number', 'mth_ppl_netcash_queries_number', 'month_h2h_mnty_tr_number', 'mth_ext_pos_mnty_tr_number', 'current_payroll_incm_amount', 'saving_prdt_avg_bal_amount', 'consumer_cash_balance_amount', 'bbva_payrl_oblg_tot_bal_amount', 'avg_cr_card_balance_amount', 'new_segment_id', 'cr_dbt_cd_digital_expns_amount', 'cr_dbt_cd_courier_expns_amount', 'cr_dbt_cd_rstrnt_expns_amount', 'cr_dbt_card_spmk_expns_amount', 'cr_dbt_card_tech_expns_amount', 'town_id']\n"
          ]
        }
      ],
      "source": [
        "selected_cols = best_num_cols + best_cat_cols\n",
        "print('Columnas seleccionadas para el modelo final:', selected_cols)\n",
        "\n",
        "numeric_transformer_sel = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer_sel = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor_sel = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer_sel, [col for col in selected_cols if col in num_cols]),\n",
        "    ('cat', categorical_transformer_sel, [col for col in selected_cols if col in cat_cols])\n",
        "])\n",
        "\n",
        "# Pipeline final sin SMOTE (si se requiere)\n",
        "clf_final = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor_sel),\n",
        "    ('classifier', lgb.LGBMClassifier(n_estimators=100, random_state=42))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "data_split_training",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 8627, number of negative: 8627\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009005 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 10266\n",
            "[LightGBM] [Info] Number of data points in the train set: 17254, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Average Precision (SMOTE): 0.6440572153131335\n",
            "Precisión: 0.9313358302122348\n",
            "F1 Score (Clase 1): 0.5714285714285714\n"
          ]
        }
      ],
      "source": [
        "X_selected = X[selected_cols]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf_final_smote = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor_sel),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', lgb.LGBMClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "clf_final_smote.fit(X_train, y_train)\n",
        "\n",
        "y_pred_smote = clf_final_smote.predict(X_test)\n",
        "\n",
        "# Calcular average_precision_score y precisión\n",
        "ap_smote = average_precision_score(y_test, clf_final_smote.predict_proba(X_test)[:, 1])\n",
        "accuracy = accuracy_score(y_test, y_pred_smote)\n",
        "f1_class1 = f1_score(y_test, y_pred_smote, pos_label=1)\n",
        "\n",
        "print(\"Average Precision (SMOTE):\", ap_smote)\n",
        "print(\"Precisión:\", accuracy)\n",
        "print(\"F1 Score (Clase 1):\", f1_class1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "evaluation",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score (Clase 1): 0.5714285714285714\n",
            "Matriz de Confusión ordenada:\n",
            "           Predicho: 0  Predicho: 1\n",
            "Actual: 0         2128           50\n",
            "Actual: 1          115          110\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "\n",
        "# Calcular el F1 score para la clase 1\n",
        "f1_class1 = f1_score(y_test, y_pred_smote, pos_label=1)\n",
        "print(\"F1 Score (Clase 1):\", f1_class1)\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_smote)\n",
        "# Crear un DataFrame con etiquetas ordenadas para la matriz\n",
        "conf_df = pd.DataFrame(conf_matrix, index=[\"Actual: 0\", \"Actual: 1\"], columns=[\"Predicho: 0\", \"Predicho: 1\"])\n",
        "print(\"Matriz de Confusión ordenada:\")\n",
        "print(conf_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "predictions_intro",
      "metadata": {},
      "source": [
        "## Generación de Predicciones y Archivo de Salida\n",
        "\n",
        "Se generan las predicciones sobre el conjunto de validación y se guarda el resultado en un archivo CSV con las columnas:\n",
        "- **ID_cliente**\n",
        "- **alta_tdc** (1 si el cliente adquiere la tarjeta, 0 en caso contrario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "generate_predictions",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivo de predicciones generado: predicciones.csv\n"
          ]
        }
      ],
      "source": [
        "validation_df = pd.read_csv('Validation_df.csv', delimiter=';', thousands='.', decimal=',')\n",
        "\n",
        "X_validation = validation_df[selected_cols]\n",
        "\n",
        "validation_preds = clf_final_smote.predict(X_validation)\n",
        "\n",
        "output_df = pd.DataFrame({\n",
        "    'ID_cliente': validation_df[id_col],\n",
        "    'alta_tdc': validation_preds\n",
        "})\n",
        "\n",
        "output_df.to_csv('predicciones.csv', index=False, sep=';')\n",
        "print('Archivo de predicciones generado: predicciones.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {},
      "source": [
        "## Conclusión y Próximos Pasos\n",
        "\n",
        "**Decisiones tomadas:**\n",
        "\n",
        "- Se entrenó un modelo base con LightGBM para obtener la importancia de las variables y se optimizó el umbral para seleccionar las más relevantes.\n",
        "- Se utiliza un pipeline final que incluye balanceo de clases mediante SMOTE y LightGBM como clasificador.\n",
        "- Se imprimen únicamente el average_precision_score y la precisión final del modelo.\n",
        "\n",
        "**Próximos pasos:**\n",
        "\n",
        "- Explorar la optimización de hiperparámetros para LightGBM.\n",
        "- Evaluar otros algoritmos (como XGBoost o CatBoost) y compararlos.\n",
        "- Profundizar en técnicas de validación cruzada y análisis de sensibilidad del modelo."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
