{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c9a5fa",
   "metadata": {},
   "source": [
    "# Predicción de Altas de Tarjetas de Crédito\n",
    "### Prueba Técnica\n",
    "\n",
    "En este notebook se desarrolla un modelo de Machine Learning para predecir si un cliente adquirirá una tarjeta de crédito (alta_tdc). Se utilizan los archivos `train_df.csv` para el entrenamiento y `Validation_df.csv` para la generación de las predicciones finales. Además, se documentan las decisiones tomadas y se reflexiona sobre posibles mejoras futuras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e2110",
   "metadata": {},
   "source": [
    "## Objetivo y Entregables\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "El objetivo de esta prueba es desarrollar un modelo capaz de predecir si un cliente adquirirá una tarjeta de crédito, utilizando el dataset proporcionado y aplicando técnicas de preprocesamiento y modelado.\n",
    "\n",
    "### Entregables\n",
    "\n",
    "- Notebook con el desarrollo del modelo, que incluya:\n",
    "  - Desarrollo del modelo\n",
    "  - Justificación de las decisiones tomadas\n",
    "  - Reflexión sobre mejoras o posibles próximos pasos\n",
    "- Archivo de predicciones (.csv) con las columnas `ID_cliente` y `alta_tdc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25452b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Lectura del CSV de entrenamiento\n",
    "train_df = pd.read_csv('train_df.csv', delimiter=';', thousands='.', decimal=',')\n",
    "\n",
    "# Visualizamos las primeras filas del dataset\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3572e4b",
   "metadata": {},
   "source": [
    "## Exploración y Preprocesamiento de Datos\n",
    "\n",
    "En esta sección se separan las variables en numéricas y categóricas. Se observa que el dataset contiene una columna de identificación (`identificador_cliente`) y la variable objetivo (`alta_tdc`), por lo que estas no se utilizarán como características.\n",
    "\n",
    "Se realiza la conversión de las columnas categóricas a tipo `str` para evitar problemas de mezcla de tipos y se procede a la preparación de pipelines específicos para cada tipo de dato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1b1079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir variables objetivo e identificación\n",
    "target = 'alta_tdc'\n",
    "id_col = 'identificador_cliente'\n",
    "\n",
    "# Separar características (X) y variable objetivo (y)\n",
    "X = train_df.drop([target, id_col], axis=1)\n",
    "y = train_df[target]\n",
    "\n",
    "# Seleccionar columnas numéricas y categóricas\n",
    "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "print(\"Columnas numéricas:\", num_cols)\n",
    "print(\"Columnas categóricas (antes de conversión):\", cat_cols)\n",
    "\n",
    "# Convertir columnas categóricas a string\n",
    "X[cat_cols] = X[cat_cols].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f6a5fa",
   "metadata": {},
   "source": [
    "## Construcción del Pipeline de Modelado\n",
    "\n",
    "Para estructurar el proceso de preprocesamiento y modelado se han definido dos pipelines:\n",
    "\n",
    "- **Pipeline para variables numéricas:**\n",
    "  - Imputación con la mediana (estrategia robusta ante valores atípicos).\n",
    "  - Escalado con `StandardScaler` para normalizar las características.\n",
    "\n",
    "- **Pipeline para variables categóricas:**\n",
    "  - Imputación con la moda para rellenar valores faltantes.\n",
    "  - Transformación a variables dummy (One-Hot Encoding) con manejo de categorías desconocidas.\n",
    "\n",
    "Estos pipelines se combinan mediante un `ColumnTransformer`, y se integran en un pipeline final que incluye un clasificador de Bosques Aleatorios (RandomForestClassifier) configurado con 100 estimadores y una semilla fija para garantizar reproducibilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b3a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline para variables numéricas\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline para variables categóricas\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combinar ambos pipelines con ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_cols),\n",
    "    ('cat', categorical_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "# Pipeline completo: preprocesamiento + modelo\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd48e1c",
   "metadata": {},
   "source": [
    "## Entrenamiento y Evaluación del Modelo\n",
    "\n",
    "Se realiza la división del dataset de entrenamiento en conjuntos de entrenamiento y prueba (80-20). Posteriormente, se entrena el modelo completo y se evalúa mediante la obtención de un reporte de clasificación y la precisión global.\n",
    "\n",
    "El reporte obtenido es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3413f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# División en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicción y evaluación\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"\\nReporte de clasificación:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Precisión:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c854047",
   "metadata": {},
   "source": [
    "Se observa que el modelo tiene una alta precisión global, aunque la clase minoritaria (alta_tdc = 1) presenta un recall más bajo, lo que indica posibles mejoras en la detección de dicha clase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d848b0bb",
   "metadata": {},
   "source": [
    "## Justificación de las Decisiones Tomadas\n",
    "\n",
    "- **Imputación y escalado para variables numéricas:** La estrategia de imputar con la mediana es robusta ante valores atípicos. El escalado permite que las variables numéricas estén en la misma escala, lo cual es importante para algoritmos basados en distancia y para la convergencia de algunos modelos.\n",
    "\n",
    "- **Imputación y One-Hot Encoding para variables categóricas:** Utilizamos la moda para rellenar los valores faltantes en variables categóricas, y el One-Hot Encoding permite representar estas variables de forma adecuada sin asumir un orden implícito.\n",
    "\n",
    "- **Selección del clasificador:** Se optó por un RandomForestClassifier por su capacidad para manejar conjuntos de datos heterogéneos, su robustez frente al overfitting y su interpretabilidad parcial a través de la importancia de variables.\n",
    "\n",
    "- **División de los datos:** La separación en conjuntos de entrenamiento y prueba (80-20) permite evaluar el desempeño del modelo en datos no vistos y estimar su capacidad de generalización.\n",
    "\n",
    "El reporte de clasificación sugiere que el modelo tiene un buen desempeño global, aunque se podría trabajar en mejorar la detección de la clase minoritaria (alta_tdc = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f3343",
   "metadata": {},
   "source": [
    "## Reflexión y Próximos Pasos\n",
    "\n",
    "Si bien los resultados obtenidos son prometedores, se pueden considerar las siguientes mejoras y próximos pasos:\n",
    "\n",
    "- **Optimización de Hiperparámetros:** Realizar un ajuste fino (grid search o random search) para optimizar parámetros del RandomForest (número de estimadores, profundidad máxima, etc.).\n",
    "\n",
    "- **Manejo del Desbalance de Clases:** Dado que la clase minoritaria tiene un recall inferior, se podría aplicar técnicas de sobremuestreo (por ejemplo, SMOTE) o submuestreo de la clase mayoritaria.\n",
    "\n",
    "- **Validación Cruzada:** Implementar validación cruzada para evaluar de forma más robusta la capacidad de generalización del modelo.\n",
    "\n",
    "- **Ingeniería de Variables:** Revisar el diccionario de variables para crear nuevas características o transformar las existentes que puedan aportar mayor información al modelo.\n",
    "\n",
    "- **Modelos Alternativos:** Probar otros algoritmos (por ejemplo, XGBoost, LightGBM) que podrían ofrecer mejoras en la predicción de la clase minoritaria.\n",
    "\n",
    "Estas acciones pueden ayudar a mejorar tanto la precisión como la capacidad de generalización del modelo, especialmente en la detección de clientes con alta probabilidad de adquirir la tarjeta de crédito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f13b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generación de predicciones para el conjunto de validación\n",
    "\n",
    "# Se asume que el archivo 'Validation_df.csv' tiene una estructura similar al de entrenamiento\n",
    "val_df = pd.read_csv('Validation_df.csv', delimiter=';', thousands='.', decimal=',')\n",
    "\n",
    "# Conservamos la columna de identificación\n",
    "id_val = val_df['identificador_cliente']\n",
    "\n",
    "# Para las predicciones, se eliminan las columnas que no corresponden a las características\n",
    "X_val = val_df.drop(['identificador_cliente'], axis=1)\n",
    "\n",
    "# Generar las predicciones usando el pipeline entrenado\n",
    "predicciones = clf.predict(X_val)\n",
    "\n",
    "# Crear un DataFrame con las predicciones, renombrando la columna de ID según lo requerido\n",
    "df_pred = pd.DataFrame({\n",
    "    'ID_cliente': id_val,\n",
    "    'alta_tdc': predicciones\n",
    "})\n",
    "\n",
    "# Guardar las predicciones en un archivo CSV\n",
    "df_pred.to_csv('predicciones.csv', index=False)\n",
    "print(\"Archivo de predicciones generado: predicciones.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37989c1f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
